# sequencer

Developed with Unreal Engine 5


Herausforderungen:

Für mich Persönlich waren die Größte Herausforderung sich mit einer komplett neuen Technologie auseinander zu setzten das geht bei der VR Brille los und hört bei Unreal Engine und TouchDesign auf und vorallem ein Verständnis für unser Projekt zu entwickeln. Dies liegt besonders daran, dass meine Ursprüngliche Vorstellung von Synästhesie so weit von der Realität (Yannicks Wahrnehmung) entfernt war. So dachte ich zum Beispiel, dass ein Klavier G mit einem Grünen Dreieck gekoppelt wird.
Eine weitere große Herausforderung war die Kommunikation zwischen den Teammitgliedern. Aus meiner Arbeit (Fullstack Entwicklung) bin ich es gewohnt, dass jeder im Team ein gewissens Grundverständnis für die Programmierung hat, jedoch ist das natürlich von einem Design Student und Musik Student nicht zu erwarten. Anders herum natürlich auch, wenn Yannick (Musiker) mit für Ihn Grundbegriffen aus der Musik versucht etwas auszudrücken ist es auch oft schwer dem ganzen ohne Musikwissen zu folgen. Das ganze hat vermutlich bei allen Teammitgliedern zu einer gewissen Frustration geführt.


Vorgehensweise:

Aus unserem anfänglichen Plan die Visuals nur in Unreal Engine umzusetzen wurde nach kurzer Zeit schon die Idee, dass ganze mit TouchEngine umzusetzen und die Audioreaktiven Visuals in TouchDesign zu erstellen und dann in Unreal zu streamen. Das war würde ich sagen im nachhinein unser erster großer Fehler. Denn somit haben wir uns die Aufgabe gemacht in für mich zwei neuen Technologien etwas umzusetzen was aufgrund von einem geringen Anwendungsbedarf der Community einfach schlecht Dokumentiert bzw. gar nicht dokumentiert war. Durch den Zeitdruck und der gerade beschriebenen Probleme haben wir uns dann einen Plan B zurecht gelegt. Dieser sah vor, die Visuals aus TouchDesign als Video auf unser Unreal Engine Material zu projizieren. Dies muss man sagen hat dann glücklicherweise sehr gut Funktioniert und uns in Hinsicht zur Ausstellung sehr weit gebracht. Denn wir konnten Somit am Donnerstag (4.7.) einen funktionierenden Prototypen vorstellen. (Siehe Link)


Der Prototyp:




Vision:

Für mich bieten sich bei einer Fortsetzung des Projekts ab jetzt Zwei wege an. Entweder man versucht die TouchEngine zu Implementieren oder man legt den Fokus nur auf eine Technologie wo ich auf jeden fall auf Unreal Engine zurückgreifen würde. So könnte man das ganze Schärfer, Flüssiger und Interaktiver machen. Die Herausforderung die sich aber dabei stellt die Audioreaktiven Visuals mit der (nach meiner Einschätzung) nicht für gemachten Technologie umzusetzen. Dennoch würde wie eben erwähnt das Erlebnis noch deutlich intensiver und Realistischer werden.


Meine Aufgaben:

Einrichtung der VR Brille
Umsetzung des Schnittes/Ablauf des Erlebnisses
Kreation der Objekte und Materials für die einzelnen Sphären 
Sound Räumlich an Objekte und Szenen binden
Programmieren eines Partikel (Niagara Systems)
Level-Blueprint für das abspielen der Videos programmiert
